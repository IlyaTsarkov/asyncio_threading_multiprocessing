<h3>
    aiohttp
</h3>
<p>
    Асинхронность нам в большей степени интересна со стороны web-разработки, да и в целом на начало осени 2022 весь материал на сайте сводится к вебу.<br>
    Для асинхронной web-разработки существуют отдельные фреймворки, которые только набирают популярность и не продолжать это делать, с такими фреймворками мы познакомимся
    отдельно, сейчас же хотелось бы поговорить о более низкоуровневых инструментах для асинхронной работы с http запросами.
</p>
<p>
    Для начала установим библиотеку aiohttp<br>
    <code>pip install aiohttp</code>
</p>
<p>
    <a href="https://docs.aiohttp.org/en/stable/client_quickstart.html" target="_blink" class="links">документация</a>
</p>
<p>
    Мы не станем разбираться с ней очень подробно, этот раздел нужен скорее для дополнительных примеров по работе с корутинами. Плюс хочется показать вам
    один из сайтов для тренировки по работе с http запросами.
</p>
<p>
    Существует сайт <a href="https://jsonplaceholder.typicode.com/" target="_blink" class="links">{JSON} Placeholder</a> на нем есть небольшая база данных связанных между
    собой, они уже представлены в json формате, мы можем делать к ним запросы и получать эти данные.
</p>
<p>
    Например, есть страница с <a href="https://jsonplaceholder.typicode.com/albums" target="_blink" class="links">альбомами фотографий</a>.<br>
    У каждого альбома есть id пользователя, который создал этот альбом, всего в базе 10 пользователей и 100 альбомов, по 10 на каждого, есть id самого альбома и
    есть его название.
</p>
<p>
    Как бы в самом простом формате мы могли бы забрать данные с этого сайта и вывести их в консоли? С помощью библиотеки requests.<br>
    Если ранее вы с ней не работали, то потребуется ее установить<br>
    <code>pip install requests</code>
</p>

[
{
"userId": 1,
"id": 1,
"title": "quidem molestiae enim"
},
{
"userId": 1,
"id": 2,
"title": "sunt qui excepturi placeat culpa"
},
import requests<br>
<br>
<br>
url = 'https://jsonplaceholder.typicode.com/albums'<br>
print(requests.get(url).text)
<p>
    Очень просто. Импортируем библиотеку requests, пишем url, применяем к нему метод get для получения данных и к полученным данным метод text для вывода результата.<br>
    Получаем 100 записей, все 100, конечно, вставлять в результат я не стал, формат у всех такой же как у первых двух.
</p>
<p>
    В url мы можем добавить параметр<br>
    url = 'https://jsonplaceholder.typicode.com/albums?userId=1'<br>
    добавим ?userId=1, тем самым заберем только записи пользователя с id=1, первые 10 записей.
</p>
<p>
    Таким образом мы получаем данные в json формате, чтобы преобразовать их к привычному python формату надо применить к полученному ответу метод .json()<br>
    import requests

    url = 'https://jsonplaceholder.typicode.com/albums?userId=1'
    response = requests.get(url)
    print(response.json())<br>
    prin() распечатает нам список из 10 словарей, каждый из которых содержит информацию об отдельном альбоме.
</p>
import requests


class Album:
def __init__(self, user_id, album_id, album_title):
self.user_id = user_id
self.album_id = album_id
self.album_title = album_title

@classmethod
def json_render(cls, obj):
user_id = obj['userId']
album_id = obj['id']
album_title = obj['title']
return Album(user_id, album_id, album_title)


def albums_by_user(user):
url = f'https://jsonplaceholder.typicode.com/albums?userId={user}'

response = requests.get(url)
albums_json = response.json()

return [Album.json_render(album) for album in albums_json]


print([album.album_title for album in albums_by_user(1)])
<p>
    Можно добавить объект каждого отдельного альбома в виде экземпляра класса Album.
    В этом же классе добавим метод json_render для создания объекта класса из полученного с get запросом словаря с информацией об альбоме.<br>
    Сам запрос снесем в функцию albums_by_user, куда можно будет передать любого пользователя и забрать его альбомы, а
    возвращать функция будет список экземпляров класса Album.
</p>
<p>
    На печать выведем список названий альбомов.
</p>
<p>
    import requests


    class Album:
    """
    Класс для объявления альбомов.
    Источник https://jsonplaceholder.typicode.com/albums
    Каждый альбом содержит 3 параметра.
    """
    def __init__(self, user_id, album_id, album_title):
    self.user_id = user_id
    self.album_id = album_id
    self.album_title = album_title

    @classmethod
    def json_render(cls, obj):
    """
    Создаем экземпляр класса.
    Данные для переменных забираем из словаря
    полученных с запросом данных
    """
    user_id = obj['userId']
    album_id = obj['id']
    album_title = obj['title']
    return Album(user_id, album_id, album_title)


    def albums_by_user(user):
    """
    Функция для запроса к альбомам, забирает альбомы переданного пользователя.
    Полученные сырые данные преобразуем к списку словарей
    и из каждого словаря создаем экземпляр класса Album методом json_render()
    """
    url = f'https://jsonplaceholder.typicode.com/albums?userId={user}'

    response = requests.get(url)
    albums_json = response.json()

    return [Album.json_render(album) for album in albums_json]


    def show_albums(albums):
    """Выводим заголовки полученных альбомов"""
    for album in albums:
    print(f'{album.album_title}')


    def main():
    """Запускаем сбор альбомов пользователя с id=1"""
    albums = albums_by_user(1)
    show_albums(albums)


    if __name__ == '__main__':
    main()
</p>
<p>
    Добавим программе еще немного структурированности и комментариев, такое количество комментариев для такой программы излишне, когда речь идет о реальных программах,
    но работа с запросами важный навык и дополнительное объяснение в учебном материале лишним не будет.
</p>
<p>
    Думаю разжевано все максимально подробно.
</p>
<p>
    Теперь мы можем передать в albums_by_user() 10 пользователей поочередно и совершить 10 поочередных запросов.
</p>
def main():
"""Запускаем сбор альбомов пользователя с id=(range(10))"""
n = 1
while n <= 10:
albums = albums_by_user(n)
show_albums(albums)
n += 1
<p>
    Допустим вот так.
</p>
<p>
    Подключаем библиотеку aiohttp, очень грубо говоря, асинхронный аналог requests.
</p>
import asyncio
import aiohttp


class Album:
"""
Класс для объявления альбомов.
Источник https://jsonplaceholder.typicode.com/albums
Каждый альбом содержит 3 параметра.
"""

def __init__(self, user_id, album_id, album_title):
self.user_id = user_id
self.album_id = album_id
self.album_title = album_title

@classmethod
def json_render(cls, obj):
"""
Создаем экземпляр класса.
Данные для переменных забираем из словаря
полученных с запросом данных
"""
return Album(obj['userId'], obj['id'], obj['title'])


def show_albums(albums):
"""Выводим заголовки полученных альбомов"""
for album in albums:
print(f'{album.album_title}')


async def main():
async with aiohttp.ClientSession() as session:
async with session.get('https://jsonplaceholder.typicode.com/albums?userId=1') as resp:
albums_json = await resp.json()
albums = [Album.json_render(album) for album in albums_json]
show_albums(albums)


if __name__ == '__main__':
asyncio.run(main())
<p>
    Делает эта программа точно то же самое, что и ее неасихнронная версия, просто я записал ее покомпактнее, всю асинхронность вынес в корутин main().
    Строки<br>
    async with aiohttp.ClientSession() as session:<br>
    async with session.get(f'https://jsonplaceholder.typicode.com/albums?userId=1') as resp:<br>
    взяты с первой страницы документации, async with дает нам гарантию, что запрос не только не только не заблокируется, но и полностью завершиться.
    aiohttp.ClientSession поддерживает одновременное подключение к 100 серверам и от этой сессии мы в примере обратились к {JSON} Placeholder, соответственное
    только от этой сессии мы можем подключиться еще к 99 серверам и делать к ним запросы асинхронно.<br>
    Подробнее об этом на
    <a href="https://docs.aiohttp.org/en/stable/http_request_lifecycle.html#aiohttp-request-lifecycle" target="_blink" class="links">этой странице </a>
    документации.
</p>
<p>
    Зная это можем обратиться от сессии одновременно к альбомам первых 3-х авторов.
</p>
...
async def main():
async with aiohttp.ClientSession() as session:
async with session.get('https://jsonplaceholder.typicode.com/albums?userId=1') as resp:
albums_json = await resp.json()
albums = [Album.json_render(album) for album in albums_json]
show_albums(albums)
async with session.get('https://jsonplaceholder.typicode.com/albums?userId=2') as resp:
albums_json = await resp.json()
albums = [Album.json_render(album) for album in albums_json]
show_albums(albums)
async with session.get('https://jsonplaceholder.typicode.com/albums?userId=3') as resp:
albums_json = await resp.json()
albums = [Album.json_render(album) for album in albums_json]
show_albums(albums)
...
<p>
    Таким образом три запроса к альбомам разных авторов выполняются параллельно и результат мы получаем мгновенно.
</p>

<h3>
    Порядок вывода. Tasks
</h3>
<p>
    Когда речь заходит об асинхронности, то скорее всего она будет использована для отдельных задач. Нам хочется, чтобы одни процессы происходили отдельно от других, не мешали друг другу,
    не блокировали друг друга, для этого мы можем вынести их в отдельную задачу и сказать ей исполнятся по команде, тогда когда нам это будет нужно.
    Для работ с задачами есть инструмент под названием Celery, но разговор о Celery тема отдельного материала, тем не менее Celery и asincio могут работать вместе и
    упомянуть о возможности создавать задачи средствами asincio считаю нужным занятием.
</p>
<p>
    Первый вопрос, который хочется озвучить: имеет ли значение порядок ключевых слов await для вывода разных корутин?
</p>
<p>
    Сразу посмотрим на примере.
</p>


<p>
    Конечно порядок имеет значение. Пусть программирование у нас в данном случае и асинхронное, но исполнение кода сверху вниз никто не отменял.
</p>
<p>
    Напишем три корутины, но две будут исполняться мгновенно, а одна будет имитировать задержу в несколько секунд. Результат будем получать в функции main(), ничего не
    мешает использовать несколько ключевых слов await дял вывода разных функций. Таким образом при запуске данной программы все await запустятся последовательно.
</p>

Сообщение 1
2
Старт задачи Задача 1
Конец задачи Задача 1
Сообщение 2
2

Process finished with exit code 0

<p>
    Вывод ожидаемый, но тут становится очевидна проблема, два последних исполнения мгновенноисполняемых корутин не начнутся пока мы не дождемся окончания выполнения корутины с задержкой.
    Зачем нам ждать пока она завершится? Пусть сначала выполняться мгновенные корутины, а потом остальные. И решение очевидно, поменять выводы местами и просто поставить работу более долгой
    операции в конец. Но есть другой вариант, создать задачу на основе этого корутина и также вызвать ее в конце.
</p>

Сообщение 1
2
Сообщение 2
2
Старт задачи Задача 1
Конец задачи Задача 1

Process finished with exit code 0
<p>
    Создается задача методом .create_task(), в качестве аргумента принимает функцию корутин. К объекту типа Task также применимо ключевое слово await, поскольку Task это awaitable объект,
    который на самом деле очень родственен с корутином. Просто использование задачи добавляет некоторую гибкость и компактность, когда речь идет именно о вызове и сборе данных из вызова,
    и конечно объекты типа Task имеют свое api, часть их котого мы рассмотрим.
</p>
<p>
    Я упомянул термин awaitable объект. Это объекты, к которым мы можем применить ключевое слово await, то есть ожидающий результат. И в asincio существует три типа объектов, к которым
    применимо слово await. Два из них уже были упомянуты - coroutine и task, и существует еще один тип - future. Future более низкоуровневый чем Task тип и взаимодействие напрямую с
    ним чаще всего не имеет большого смысла. Пока задачи типа Task и Future исполняются, они меняют свои состояния. Задача может быть еще не запущена(Pending), запущена(Running), завершена(Done)
    и отменена(Cancelled). На работе с этими состояниями и завязана особенность в работе с задачами, заключается он в том, что из одного состояния забрать результат мы можем, а из
    другого нет и манипулирование этим пониманием предоставляет возможность низкоуровневого сильно контролируемого взаимодействия с задачами.
</p>
<p>
    Давайте добавим парочку долгих задач.
</p>
<p>
    Создадим две задачи долгоисполняемых корутин и обернем их в метод .gather(), который принимает в себя любые awaitable объект. Но особенность метода .gather() в том,
    что он исполняет переданные в него awaitable объекты в том порядке, в котором они перечислены. То есть мы можем все наши корутины обернуть в задачи и в том же порядке передать в .gather().
</p>
<p>
    Тоже самое, что и без использования .gather(). Но что если мы не хотим следить за этим порядком, допустим мы не знаем, что будет исполнено быстрее, а задача заключается, например,
    в получении ответа сразу как это становится возможно. И для такой задачи есть отдельный метод, который называется .as_completed().
</p>
<p>
    Метод .as_completed() будет возвращать результат сразу, когда это становится возможным, таким образом нам не нужно следить за порядком передачи задач в этот метод. Очень удобно
    и компактно, просто в цикле перебираем итератор из awaitable объектов и возвращаем результат, когда это возможно.
</p>
<p>
    И осталась еще одна возможность вывода результата, выше я упомянул о четырех состояниях, и этот метод возвращает нам множества awaitable объектов в состояниях done и pending.
</p>
<p>
    Метод wait(). Кау упомянулось выше возвращает два множества, в данном случае все задачи выполнены, поэтому множество pending пустое.
</p>
<p>
    И последнее, состояния задачи мы можем явно просматривать, и явно на него влиять, для этого существуют отдельные методы.
</p>
<p>
    В качестве примера используем задачу short_task_22 и применим к ней некоторые методы.<br>
    Метод done() - возвращает True, если задача в состоянии Done.<br>
    Метод cancelled() соответственно возвращает True, если задача была отменена.<br>
    Метод cancel() принудительно переводит задачу в состояние cancelled.
</p>
<p>
    Как видим состояние cancelled после метода .wait() изменилось, а done осталось прежним. Если посмотреть множество done, то мы увидим там Task-7, которая в отличие от
    остальных задач начинается со строки Task cancelled...<br>
    И по скольку состояние cancelled принимается задачей после done, то и при втором применении .done() к задаче мы видим True. Но на что тогда влияет отмена?<br>
    Из отмененной задачи мы не сможем забрать результат.
</p>
<p>
    К задачам в состоянии Done мы можем применить метод .result() и посмотреть результат, но к задачам в состоянии cancelled, при применении метода .result() мы получим исключение CancelledError.<br>
    В блоке try except мы можем обработать это исключение.
</p>
<p>
    Можно было бы обсудить еще api обработчика событий event_loop, но там не так много возможностей, скорее всего использование метода .run() будет достаточно.
</p>
<p>
    Таким образом мы разобрались с задачами, состояниями, возможностями обработки результатов awaitable объектов, мы познакомились не со всем доступным api, с остальными методами вы
    всегда можете самостоятельно ознакомиться в документации. Этот материал введение в асинхронность и надеюсь он сформировал представление о возможностях этой концепции и
    конкретно о возможностях библиотеки asincio.
</p>
<p>
    Теперь давайте подробнее поговорим о такой вещи как потоки и многопоточность.
</p>
<h3>
    GIL
</h3>
<p>
    В введении этого материала была такая фраза<br>
    "...только вот мы тут говорим в первую очередь о python и python не поддерживает параллельную работу потоков..."<br>
    Это абсолютная правда, когда мы говорим о python про параллельную работу потоков внутри одного процесса мы можем забыть, по крайней мере, на начало осени 2022.
    Работа потоков в python выполняется только последовательно и все из-за такого механизма как GIL или Global Interpreter Lock.
</p>
<p>
    GIL это некая, пусть будет, переменная, которая вводит правило - 'В любой момент времени может работать только один поток и разрешение на работу этому потоку выдаю я'.
</p>
<p>
    Таким образом по настоящему параллельная работа поток не возможна.
</p>
<p>
    Разговоры об удалении GIL из python ведутся, насколько я вычитал с начал двухтысячных, а возможно и еще раньше. Создатель Python, Guido van Rossum в 2007 сказал
    в какой-то статье, что GIL будет удален из python тогда, когда производительность однопоточных приложений будет настолько же высока без GIL как с ним. Прошло 15 лет, а
    GIL все еще существует, значит плюсов от него больше чем минусов. Говоря поверхностно, минус - нет параллельно работающих потоков, плюс - однопточные приложения
    производительнее и мы имеем меньшую вероятность столкнуться с потокозависимыми проблемами в однопоточных приложениях. Это все на самом деле нам не столь важно,
    главное во всем этом:<br>
    1. факт наличия такой вещи как GIL <br>
    2. понимание какие ограничения этот GIL на нас накладывает.
</p>
<p>
    И еще на что хотелось бы обратить внимание в этом вступлении - работа с потоками интересна нам в рамках CPU-bound операций, поэтому GIL не сильно влияет на работу I/O-bound
    операций.
</p>
<p>
    То что мы не можем работать с потоками параллельно совсем не означает, что работа с потоками для нас бесполезно, напротив на работе с потоками завязано очень многое в
    современных программах. И первое о чем следует упомянуть это, конечно, как создавать поток и какие типы потоков вообще существует.
    Эти вопросы взаимосвязаны и для демонстрации предлагаю сразу перейти к примерам.
</p>
<p>
    Для демонстрации напишем любую программу, которая исполняется не мгновенно, а хотя бы пару секунд, этого будет достаточно.
    <br> Итак, перед нами классическая однопоточная программа, программа всегда запускается в потоке и это нам не нужно объявлять явно. В данном примере поток
    запускается при запуске программы, то есть внутри конструкции if __name__ == '__main__'. И программа будет работать до тех пор, пока все задачи внутри этого потока не будут выполнены.
    Конструкции if __name__ == '__main__' в данном случае главный или Foreground поток. Запомните, программа не будет завершена до тех пор, пока активен Foreground поток.
</p>
<p>
    И Foreground поток это тип, который создается по умолчанию при создании потока.
</p>
<p>
    Какая проблема существует у этой программы? Мы не видим строчку 'Завершение потока main()' пока не будет выполнена относительно долгая программа до вызова этой строки.<br>
    Давайте уберем выполнение этой долгой программы в отдельный поток и запустим его.
</p>
<p>
    Для создания потоков используется библиотека из стандартного набора - threading. Для создания потока используется threading.Thread().<br>
    Так создадим поток thread, в аргумент target= передается программа, которую мы хотим вынести в отдельный поток, в параметр args= передаются аргументы этой программы.<br>
    Для запуска потока используется метод .start().
</p>
<p>
    Поток с именем thread - Foreground поток, это означает, что пока он не будет выполнен программа не закончится, как мы видим, именно так и происходит.
    Но теперь у нас действительно работает два отдельных потока и какой тип тогда у потока main()? Такой же, как и был - Foreground.
    Мы можем иметь в одной программе несколько Foreground потоков и пока хотя бы один из них работает программа также работает.
</p>
<p>
    В данном примере это не очевидно, поток main() выполняется значительно быстрее, чем поток thread. Пусть будет наоборот.
</p>
<p>
    Для этого добавим всего одну строку - time.sleep(5). Обратите внимание, эта пауза относится именно к потоку main(), на работу потока thread она никак не влияет.
</p>
<p>
    Как мы видим, несмотря на то, что thread выполняется теперь быстрее, пока main() не завершиться программа работает, это доказывает, что оба потока имеют тип Foreground.
</p>
<p>
    Второй тип потока - Background. Отличается он от Foreground, как вы наверное уже догадались тем, что если к моменту завершения последнего Foreground потока будут незавершенные
    Background потоки, программа все равно завершиться. Давайте убедимся в этом.
</p>
<p>
    Для создания потока типа Background используется параметр daemon в значении True. Теперь исполнение потока thread даже не успевает начаться, потому что вывод двух print() потока main()
    происходит быстрее и как только последняя команда Foreground потока завершена, завершается работа всей программы и наличие незавершенных Background потоков ни на что не влияют.
</p>
<p>
    А если мы все таки хотим, чтобы программа не завершалась и дождалась выполнения не всех, а какого-нибудь явно указанного Background. Вдруг задача этого потока не настолько важная,
    чтобы делать этот поток Foreground поток, но в каком-то конкретном случае нам все таки необходимо дождаться ее исполнения.
</p>
<p>
    Для этой цели используется метод .join(). Теперь даже несмотря на то, что Foreground поток завершился программа дожидается окончания Background потока, к которому был применен метод .join().
    При чем вызвать метод .join() можно и после последней команды Foreground потока.
</p>
<p>
    Важно понимать, использование нескольких потоков внутри одного процесса только замедляет работу программу, потому что факт последовательного выполнения,
    при запуске нескольких потоков, остается, но при этом появляется переключение между этими потоками (context switch), которое также занимает время.<br>
    Поэтому потоки мы используем только для контроля над порядком исполнения блокирующих функций, но точно не для скорости работы всей программы.
</p>
<p>
    Убедиться, что 'параллельные' потоки работают медленнее последовательных очень просто. Напишем для этого следующую программу.
</p>
<p>
    Напишем функцию write_in_file(), в которой будем записывать в файл три миллиона трехзначных чисел. Напишем классический декоратор для замера времени how_long().<br>
    Будем использовать запись в 4 файла, в два из них будем записывать числа с помощью условно параллельных потоков, а в оставшиеся два с помощью классической последовательной работы потоков.<br>
    Из примера прекрасно видно, что последовательные потоки работают быстрее, при чем разница во времени будет увеличиваться с увеличением числа потоков, поскольку context switch будет происходить больше раз.
</p>
<p>
    И есть еще один момент, который мы сразу тут же можем обсудить. Обратите внимание на запуск потоков в example_with_threads(), два .start(), два .join(), а это всего два
    потока, допустим их будем 5, сколько лишних одинаковых строк придется писать. Это захламление настолько бросается в глаза, что кажется для избавления от него точно должно быть
    какое-то решение, как в случае с asincio и множественным использованием await. И такое решение действительно есть.
</p>
<p>
    Решение это называется - ThreadPoolExecutor() из concurrent.futures. В качестве аргумента передаем max_workers, это как раз количество потоков. При чем это количество
    необязательно должно быть равным количеству выполняемых задач, если потоков будет больше чем задач, то лишние потоки просто не будут задействованы, а если меньше,
    допустим мы выделили также два потока, а задач у нас три, тогда сначала выполняться две задачи, а после того как один из потоков освободится - выполнится третья задача.<br>
    Параметр max_workers можно не указывать явно, тогда это число будет равно количеству ядер устройства + 4, но не более 32.
</p>
<p>
    Открываем мы всю эту конструкцию в менеджере контекста, поскольку созданный таким образом объект закрывается функцией shutdown(). И чтобы не забыть закрыть этот объект
    мы используем with. Не использовать with нам хочется только в том случае, когда мы хотим в shutdown() хотим передать параметр wait в значении False, по умолчанию True, примерно тоже самое, что .join().<br>
    Методом .submit() мы запускаем задачу в этой созданной многопоточной среде.<br>
    Как видно из результата на время это никак не влияет, оно все равно выше, но использование ThreadPoolExecutor() добавляет компактности нашему коду.
</p>
<h3>
    Timer
</h3>
<p>
    Итак, мы разобрались, что целью использования потоков не является ускорение работы программы, целью является скорее своего рода всевозможная синхронизация этих самых потоков,
    запуск разных задач внутри разных потоков и контроль над порядком и взаимосвязями их выполнения.
</p>
<p>
    Синхронизация потоков и контроль над ними представленными несколькими классами, на каждый из них мы совсем скоро посмотрим. Правда перед этим давайте взглянем на один класс,
    который представляет собой скорее отдельную группу и не связан напрямую с синхронизацией, но когда речь идет о работе с потоками, как будто, именно такой класс напрашивается как
    пример самой простой реализации задач, которые берут на себя потоки.
</p>
<p>
    Класс Timer(). Не станем долго на нем задерживаться, одного примера будет достаточно.
</p>
<p>
    Особенность класса Timer() в параметре interval, внутри него в секундах записывается время, через которое должно начаться выполнение переданной в аргумент function функцию.
</p>
<p>
    Например, нам нужно как в примере сымитировать время ожидания какого-то действия, с помощью класса Timer() реализовать такое поведение можно очень просто.
</p>
<p>
    Класс Timer() имеет метод cancel(), с помощью него можно принудительно прервать функцию запущенную через Timer(), но только при том условии, что выполнение функции еще не началось,
    то есть время внутри interval еще не вышло.
</p>
<h3>
    Event
</h3>
<p>
    Теперь можно перейти к теме синхронизации потоков. Потоки исполняют какие-то функции, а эти функции в свою очередь используют какие-то данные и бывает, что функциям из разных
    потоков может потребоваться доступ к этим данных и такое поведение может повлечь за собой ряд проблем, о которых мы поговорим позже, но факт тут в том, что для корректной
    работы такого поведения нам нужны инструменты для взаимодействия между потоками. Инструменты синхронизации потоков.
</p>
<p>
    Первый класс, который мы рассмотрим - Event()(событие). С помощью событий мы можем оповещать выполнены какие-нибудь условия внутри потока или нет.
</p>
<p>
    Создадим простой пример.
</p>
<p>
    Для работы с классом Event() создадим его экземпляр.<br>
    Особенность Event() заключается в флаге этого события, который может быть только в состоянии True либо False. Когда мы создаем
    экземпляр класса Event() этот флаг устанавливается в состояние False. Этот флаг нам нужен для манипулирования выполнения задач с помощью метода .wait(),
    у .wait() есть только одно правило - 'пока экземпляр класса Event() находится в состоянии False все, что находится ниже .wait() выполнено не будет, пока состояние не изменится на True.'.<br>
    Изменять состояние можно методом .set(), а для проверки состояния существует метод .is_set(), который вернет True, если состояние экземпляра True.<br>
    А если нам нужно сбросить состояние обратно в False, то используем метод .clear().
</p>
<p>
    В примере создаем программу, которая имитирует отправку подарков, например, на email, и допустим мы хотим сначала все эти подарки сформировать и только, когда все они будут готовы отправить их.
    С помощью Event() как раз можно это реализовать, мы создаем 5 потоков, каждый из которых берет на исполнение функцию gifts_prepare(), но сразу при заходе в эту функцию поток
    сталкивается с .wait(), проверяет его состояние и получает False, потому что изначально Event() создается в этом состоянии. Ниже мы делаем проверку активных потоков и как
    только их количество достигает 5, то .wait() пропускает наши потоки дальше и они успешно исполняются.
</p>
<p>
    Если мы хотим ограничить ожидание временем, то можем использовать параметр timeout (по умолчанию None) метода .wait(), в него можно передать время,
    через которое блокировка спадет автоматически.
</p>
<p>
    Например, давайте добавим задержку после формирования каждого подарка, а для .wait() установим время в 3 секунды, таким образом формирование 5 потоков и естественного
    перевода события в состояние True будет происходить дольше, чем пройдет вручную заданный таймер.<br>
    И на примере мы действительно видим, что подарки стали отправляться после третьего, а не после пятого.
</p>
<h3>
    Semaphore
</h3>
<p>
    Еще один класс для управления потоков - Semaphore(). Особенность в том, что мы можем задать количество мест для потоков и пока эти места заняты остальные потоки вынуждены ждать пока место освободится.
</p>

<p>
    Экземпляр класса Semaphore() имеет параметр value, по умолчанию этот параметр равен 1, это как раз то самое допустимое количество мест для потоков.
</p>
<p>
    Если мы хотим указать, что место занято используем метод acquire(), а для того, чтобы освободить место используем метод release(). В release() можно передать параметр n, в
    котором указывается количество высвобождаемых потоков. А для acquire() можно задать параметр timeout, в котором явно указывается время, через которое место освободится.
</p>
<p>
    Таким образом в примере мы формируем условие, что одновременно могут формироваться только два подарка, таким образом все остальные победители попадают в
    начало функции gift_send(), мы видим вывод фразы 'Оповещаем победителя {winner}' и ждем, когда освободится место для формирования подарка, а освободится оно тогда, когда сформированный
    подарок будет отправлен.
</p>
<p>
    У Semaphore() есть дочерний класс - BoundedSemaphore(). Разница у них следующая. Как упоминалось выше у .release() есть параметр n, куда мы можем передать, например, значение 2,
    таким образом даже в нашем примере после нескольких итераций количество свободных мест станет выше, чем значение value. И как раз если мы хотим избежать такого поведения,
    то вместо Semaphore() следует использовать BoundedSemaphore(), который автоматически формирует ValueError, когда мест больше чем в параметре value.
</p>
<p>
    Так мы видим, что в ходе выполнения программы с параметром 2 в методе .release(), значение value было превышено дважды.
</p>
<h3>
    Barrier
</h3>
<p>
    Следующий класс для синхронизации потоков - Barrier(). Его особенность полностью отражена в названии, идея заключается в том, что мы устанавливаем некий барьер, и пока все потоки не подойдут к
    этому барьеру исполнение инструкций после барьера не начнется.
</p>
<p>
    В качестве обязательно параметра Barrier() принимает количество потоков.<br>
    Метод .wait() ставит точку, в которую должны упереться все потоки, перед тем как продолжить исполнение дальнейших инструкций.<br>
    Если необходимо вручную поставить поток в нерабочее состояние используется метод .abort(). Вызовы метода .wait() при этом завершаться ошибкой threading.BrokenBarrierError.<br>
    Для возвращения барьера в пустое состояние используется метод .reset(), его использование будет означать, что все ожидающие исполнения через .wait() потоки будут принудительно
    остановлены с вызовом ошибки threading.BrokenBarrierError.<br>
</p>
<p>
    Для просмотра состояния барьера существуют атрибуты<br>
    .parties - возвращает количество потоков, ожидающих прохождение барьера<br>
    .n_waiting - возвращает количество потоков еще не переданных в исполнение барьера.<br>
    .broken - True, если барьер в нерабочем состоянии.
</p>
<p>
    В примере мы снова имитируем отправку подарков, но в данном случае мы хотим отослать их всем победителям только после того, как все подарки будут собраны.
    Используем .randint() для имитации разного времени сбора подарка, таким образом получается, что даже если первый подарок собран за 2 секунды он все равно вынужден ждать пока будут собраны все подарки.
    В выводе мы видим, что собраны подарки в разное время, а отправлены все вместе сразу же как только был собран последний подарок.
</p>
<h3>
    Lock. RLock
</h3>
<p>
    Чаще всего для синхронизации потоков вы будете пользоваться не классами рассмотренными ниже, хотя и знать об их возможностях, конечно, лишним не будет, а классами - Lock и RLock.
    Когда речь идет о работе с несколькими потоками, которые имеют доступ к одной переменной и как-то с ней взаимодействует возникает вероятность возникновения абсолютно
    неожиданных результатов, только потому, что context switch может давать разным потокам влиять на эту переменную еще до того, как какой-нибудь из, ранее взявшихся за эту
    переменную, потоков еще не закончил свои манипуляции. Отсюда возникает необходимость в блокировке действия над переменной пока над ней работает какой-нибудь из потоков
    и только когда допущенной к переменной поток доделает все свои дела другой поток сможет приступить к своим делам над этой переменной.
</p>
<p>
    Посмотрим на пример.
</p>
<p>
    Класс ListOfUsers хранит список пользователей, который изначально содержит два имени, он может быть и пустым, это не определяющее проблему условие, просто так, наверное, более наглядно.
    У класса есть всего один метод, который добавляет в этот список новое имя. Имя я решил формировать случайной последовательность, просто программа так выглядит повеселее.
    В цикле создаются три потока, каждый использует функцию new_name и в качестве аргумента передает в нее случайно сформированное имя. В методе new_name() существует задержка,
    это самый важный момент программы. Важный потому что, если бы этой паузы не было поток мгновенно доделывал бы свои дела, а именно добавлял новое имя в список, но с
    задержкой успевает происходить context switch, который как раз и ломает порядок добавления пользователей в список. Как вы уже наверное поняли при запуске пользователи будут
    добавлять в список не в том порядке, в котором были созданы и переданы в метод new_name(), а в случайном и каждый раз этот порядок может быть разным, ну разумеется чем
    больше входных данных тем более непредсказуемый результат может сформироваться.
</p>
<p>
    Запустим программу
</p>
<p>
    Создаются пользователи в одном порядке, а добавляются в другом, таким образом если бы этот порядок был нам очень важен, то программа выполняла бы свою задачу некорректно,
    а если бы в качестве входных данных был не пополняемый список, а изменяемое одно единственное число, то результат нас бы точно не радовал и программа также работала бы не корректно.
</p>
<p>
    Думаю проблема озвучена и представлена понятно, и поскольку проблема достаточно яркая и заметная, то и решение для нее есть очень простое и лаконичное.
</p>
<p>
    Создаем блокиратор как экземпляр класса Lock(). Метод .acquire() для установки блокировки, метод .release() для снятия блокировки. Метод .acquire() поддерживает
    параметр timeout, в которое передается время, через которое блокировка будет снята, при этом неважно был ли применен метод .release() или нет.
</p>
<p>
    Теперь как видно потоки последовательно используют метод. Первый поток взялся за метод и стал применять его к переменной, спокойно
    сделал в нем все свои дела и вышел, тем самым освободив место для следующего потока, не переживая, что пока он занят тут своими делами придет другой поток, отнимет у него эту
    переменную и применит к ней метод.
</p>
<p>
    Lock() должен закрываться, метод .release() и зачастую методы, которые должны закрываться поддерживают with и Lock не исключение.
</p>
<p>
    Использовать with можно вот таким образом.
</p>
<p>
    Помимо Lock() существует RLock(). Реализация представлена также метода .acquire() и .release() и также блокирует фрагмент кода для одного потока. Отличие заключается в
    том, что RLock() решает проблему повторяющейся блокирующей блокировки. Что тут имеется ввиду, допустим есть поток блокирующий фрагмент методом .acquire() и пока фрагмент не
    освободится методом .release() новый поток не приступит к выполнению этого кода, и из-за неграмотно написанного кода может получиться ситуация, что на
    один фрагмент повесятся два .acquire() и исполнение заблокируется, до .release() мы уже никогда не дойдем. Или поток, который использует .acquire() вызывается еще
    раз в каком-то другом фрагменте, мы про это забыли, а он также повесит .acquire() и зависнет навсегда. Как раз RLock сам отлавливает такие ситуации и если
    он видит, что фрагмент сам себя заблокировал (это состояние называют Deadlock) разблокирует его и фрагмент успешно исполняется.
</p>
<p>
    В нашем примере это можно вызвать так, пусть после sleep() стоит еще одна блокировка, не важно как она могла бы там получиться, но в реальности методы сложнее чем в этих примерах,
    поэтому на этом месте мог быть вызов еще какого-нибудь метода, где использовался этот поток и мы бы оказались в такой ситуации. Все, мы увидели две строчки и программа
    будет висеть в таком состоянии пока мы ее вручную не остановим. Если заменить Lock(), на RLock() и больше ничего не менять, то программа бы без каких-либо трудностей отработала.
    Либо мы могли добавить в .acquire() параметр timeout и через время из timeout программа бы исполнилась, но предпочтительнее все-таки использовать RLock() в подобных ситуациях.
</p>
<h3>
    Condition
</h3>
<p>
    Последний класс для работы с синхронизацией потоков - Condition(). В совей реализации напоминает Event(), мы также имеем флаг True False и метод .wait(),
    фрагмент кода после которого не начнет исполняться, пока флаг не примет значение True. Но особенность в том, что как только фрагмент после .wait() выполнен, то
    флаг опять становится в значение False и мы снова должны ждать пока что-то спровоцирует переключение состояния флага.
</p>
<p>
    Пример может выглядеть так
</p>
<p>
    В классе GiftsSend создадим экземпляр Condition() и 2 вспомогательные переменные. Создадим метод who_win(), который будет проверять переданное в него имя,
    должно быть соблюдено условие, что первая буква имени заглавная, иначе победитель переходит в .wait() и если бы там не был явно указан параметр timeout, то находился бы
    он в этом .wait() пока мы вручную не остановили программу.<br>
    Вместо with можно было использовать методы .acquire() и .release().
</p>
<p>
    В результате мы видим, что сначала проверяются все имена, те что начинаются с большой буквы выдаются сразу, а те что с маленькой ждут пока будут выданы подарки всем
    заслуживающим и только после этого получают сообщение, что подарок они не получают.
</p>
<p>
    Но если timeout не задавать, то программа бы работала пока мы ее не остановили вручную, конечно, такого поведения нам бы хотелось избежать.
    Для этого немного изменим программу.
</p>
<p>
    Для снятия блокировки есть методы .notify() и .notify_all(). Добавим в программу проверку первой буквы каждого имени и будем разблокировать .wait() для таких имен,
    таким образом мы сразу сортируем все имена и сразу решаем кому отправлять подарок, а кому нет. .notify_all() разблокирует все .wait(), а .notify()
    можно передать число потоков, которое должно быть снято с .wait().
</p>
<p>
    Таким образом, мы достаточно подробно разобрались с потоками и с возможностями для взаимодействия с ними.
</p>
<p>
    Осталось последняя тема, о которой хотелось бы поговорить внутри данного материала - Многопроцессорность.
</p>
<h3>
    Многопроцессорность
</h3>
<h3>
    multiprocessing
</h3>
<p>
    Для работы с потоками используется модуль multiprocessing стандартного набора библиотек python. Процессы, в отличие от потоков, уже помогут нам повлиять на скорость работы программы.
    И многие синтаксические особенности, на самом деле, очень схожи с threading.
</p>
<p>
    Сразу напишем программу
</p>
<p>
    Создается процесс, как и поток, только вместо threading.Thread используется multiprocessing.Process, атрибуты и их назначение у потоков и процессов полностью
    одинаковые, процесс также имеет параметр daemon, также работает, если к нему применен .join() и также бере на себя выполнение программы в него переданной.<br>
    Поскольку пnew_user(роцессы напрямую связаны с нашим устройством часто для просмотра какой-то информации с процессами связанной мы будем использовать модуль os. Так в данном
    примере с помощью os мы смотрим id текущего потока, а также выводим id родительского процесса. В данном случае родительский это точка входа if __name__ == '__main__',
    процесс, который бы запускался и без явного указания, что этот процесс стоит запустить.<br>
    Имя процесса как видно формируется автоматически, если явно его не указывать. А через multiprocessing мы можем обратиться к системным атрибутам, например, вывести число ядер процессора.
</p>
<p>
    Но что самое примечательное в этом примере это пожалуй вывод результата, из него становится ясно, что класс ListOfUsers запускается 3 раза, и каждый раз новый пользователь добавляется в пустой словарь.
    Тут мы явно можем увидеть разницу между процессами и потоками. Ведь, если сейчас заменить запуск в процессах на заупск в потоках, то результат будет следующий
</p>
<p>
    Используя потоки мы запускам один процесс и внутри него 3 раза влияем на словарь пользователей, таким образом в итоге мы получаем один словарь, а не три разных.
</p>
<p>
    Поэтому, используя процессы, мы не упираемся в GIL, точнее упираемся внутри каждого процесса, но этих процессов может быть несколько,
    таким образом каждый из процессов может захватить отдельный фрагмент кода и заняться его выполнение параллельно с тем, как другие процессы будут заниматься своими фрагментами кода.
</p>
<h3>
    Queue
</h3>
<p>
    Часть примитивов взаимодействия нам знакомы из модуля threading, например Lock() в multiprocessing ведет себя также, как и в threading, за небольших исключением
    синтаксиса вызова, о котором я расскажу в примере. Но помимо знакомых примитивов multiprocessing имеет уникальные примитивы, с которыми мы ранее не сталкивались.
    Начнем знакомство с такого класса как очередь - Queue(). Мы не станем пример, воспользуемся тем же, что писали выше.
    <br> Сформулируем проблему. Используя процессы, мы запускам три разных класса и получаем три разных словаря с одной парой ключ значение в каждом, а что если бы мы хотели
    запустить три этих процесса и результат каждого класть в некое 'хранилище', из которого мы могли бы после все эти результаты забрать и объединить. И реализовать это можно как раз
    через Queue.
</p>
<p>
    Создается объект Queue() через экземпляр. В данном примере сразу посмотрим и на Queue() и Lock(). У использования подобных примитивов в multiprocessing есть
    одна особенность, используемый примитив должен быть явно передан в качестве аргумента, передаваемой в процесс функции. Как вы могли заметить q явно передан
    в метод new_user(), в качестве аргумента queue. Lock() при этом в качестве аргумента не передан, потому что создается в инициализаторе и уже существует в списке аргументов
    экземпляра. Если бы мы хотели объявить Lock() на уровне с Queue(), допустим, прямо строчкой ниже, то Lock() тоже требовалось бы явно передавать в качестве аргумента.
</p>
<p>
    Queue() умеет класть что-то в очередь - метод .put() и забирать что-то из очереди - метод .get(). В нашем примере экземпляр Queue() существует в MainProcess, поэтому класть в нее что-то
    могут все дочерние процессы, а забирать это что-то мы можем через MainProcess.
</p>
<p>
    В main мы по прежнему запускаем 3 процесса и каждый, в данном примере, передаем в список, а после запускаем, это нужно нам для того, чтобы удобно применить ко всем процессам метод .join(),
    тем самым мы говорим, что программа будет работать пока каждый этих процессов не завершится. Это нужно для корректного забирания данных из очереди, если мы не будем
    использовать .join(), мы в итоге все равно увидим словарь из трех пар, но порядок будет не такой, как задумывалось.
</p>
<p>
    Выглядеть это будет вот таким образов, без .join() мы заходим в последний цикл после первой итерации, а с .join() только после последней.
</p>
<p>
    В методе new_user() мы добавляем словарь с одной парой в очередь методом .put(), а в main в цикле с использованием метода .iter() мы это значение забираем
    с помощью .get() и с помощью .update() добавляем в новый словарь для хранения результата работы каждого из процессов.
</p>
<p>
    Таким образом мы получаем пополняемую лесенку значений словаря.
</p>
<p>
    Но как можно заметить программа не завершена, мы не видим строку Process finished with exit code 0. И завершать нам ее придется вручную. Происходит это потому что main
    процесс все еще активен. Тяжело отследить, что именно его держит в данной ситуации, но мы можем остановить его вручную для прекращения работы программы.
</p>
<p>
    Для ручного прибивания можно использовать метод .empty() для проверки наличия объектов в очереди и после того как .empty() возвращает True (то есть очередь пуста) вызывать
    raise. Можно использовать метод .close(), который провоцирует ValueError, но когда мы используем raise, можно и не писать .close() явно.
    В выводе raise мы видим что остановился MainProcess как самый главный и действующий на данный момент. Три дочерних процесса, запущенных выше
    перестают существовать как только отдают результат в .put().
</p>
<p>
    Очередь может принимать параметр maxsize, куда передается максимальное доступное элементов очереди.
</p>
<p>
    У .put() и .get() есть аналоги .put_nowait() и .get_nowait(), эти методы обрабатывают еще и отсутствие передаваемого или забираемого объекта.
    Так, используя .get_nowait(), можно переписать наш костыль завершения программы.
</p>
<p>
    При использовании .get_nowait() последняя итерация с .empty() == True будет вызывать raise Empty, мы можем ловить этот момент в try except и просто возвращать
    итоговый результат по завершению.
</p>
<h3>
    Pipe
</h3>
<p>
    Обмениваться данными между процессами можно не только через Queue(), существует еще один примитив для этих целей - Pipe()
</p>
<p>
    Особенность Pipe() в том, что возвращается кортеж каналов, один для приема данных, второй для отправки. Таким образом мы можем положить в
    Pipe() какой-то результат методом .send(), а забрать методом .recv(). Метод .send() будет применен к input каналу, а метод .recv() - к output.<br>
    Концы канал равносильны по значимости, то есть мы можем использовать в качестве input канала как первое так и второе значение кортежа, с output каналом ситуация аналогичная.
</p>
<p>
    Создаем экземпляр Pipe(), распаковываем объект в переменные и переменную, которую мы бы хотели использовать как input канал отдаем вместе с аргументами.<br>
    После отправки данных в Pipe() соединение закрывается методом .close(), этого можно в данном случе не делать, результат от этого никак не изменится,
    но если мы представим, что к Pipe() могут подключиться нежелательные процессы, то лучше, конечно, закрывать соединение после того как предполагается
    передача в Pipe() всех интересующих нас данных.
</p>
<p>
    Ну а после через канал, который предполагается как output мы эти данные можем получить.
</p>
<p>
    Можно запустить также несколько процессов и для каждого можно использовать один Pipe().<br>
    Передавать данные можно в байтовых данных, для этого используются методы .send_bytes() и .recv_bytes()<br>
    .fileno() - возвращает файловый дескриптор<br>
    .poll() - проверяет есть ли в Pipe() какие-нибудь данные доступные для чтения.
</p>
<h3>
    Pool
</h3>
<p>
    Существует класс для работы с пулом процессов - Pool(). Чем-то напоминает concurrent.futures.ThreadPoolExecutor, который мы рассматривали, когда говорили о потоках,
    у ThreadPoolExecutor есть аналог для процессов ProcessPoolExecutor, логика работы которого примерно та же. Можно использовать и его,
    но у нас есть класс от multiprocessing специально для этих же задач.
</p>
<p>
    Pool() поддерживает методы<br>
    .close() - для закрытия пула для новых задач<br>
    .join() - для ожидания завершения процесса<br>
    .terminate() - для принудительной остановки процесса
</p>
<p>
    Особенность Pool() в удобной работе с несколькими потоками.
</p>
<p>
    Создавать Poll() будем через with, в качестве аргумента передаем количество потоков, которое мы готовы выделить на данный пул. Запускать процессы из пула мы
    можем разные, используется для этого метод .apply(), куда передается функция и аргументы, при необходимости. Так напишем два статических метода и запустим их в одном
    пуле, выделив на эту задачу как раз 2 процесса. В результате мы видим, что каждый процесс выполнил отдельную задачу. Если бы мы
    передали в processes единицу, то один процесс сначала выполнил бы первый метод, а потом второй.
</p>
<p>
    .apply() имеет асинхронный аналог - .apply_async(), который поддерживает callback, он будет вызываться как только процесс завершится.
</p>
<p>
    Если мы хотим послать пул процессов не на разные задачи, а на одну, то .apply() нам, конечно, не подойдет.<br>
    Для такой задачи есть метод - .map().
</p>
<p>
    В .map() передаем функцию и аргумент, в качестве, которого ожидается итерируемый объект. Выделим на этот пул четыре процесса и в результате видим,
    что задействовано только 3.
</p>
<p>
    Также как и .apply() имеет аналог .map_async() с поддержкой callback.
</p>
<p>
    Для работы с .map_async() и .apply_async() нужно закрывать процесс и дожидаться его завершения, методы .close() и .join(). Если их не использовать,
    то вся программа завершиться с первым процессом.
</p>
<p>
    callback должен принимать вызываемый объект. Соответственно этот объект должен что-то возвращать, дополнительных методов писать не станем,
    просто возвратим результат из уже существующего, и в итоге callback возвращает нам список результатов каждого процесса.
</p>
<p>
    Pool() поддерживает также методы .imap() и .imap_unordered(). Работают они почти как .map(). Различие в том, что если мы захотим вручную выводить результат каждой
    итерации методов .map() и .imap() методом .next(), то в случае .imap() мы сможем передать параметр timeout в .next(), который вызовет TimeoutError, если результат не сможет
    быть выведен за переданное время. А .imap_unordered() отличается от .imap() тем, что будет выводить результат по мере поступления.
</p>
<p>
    И еще два доступных метода - .starmap() и .starmap_async(). В отличии от .map() в качестве итератора принимают список кортежей, которые позиционно
    распаковываются в аргументы функции. Например, функция get_result() теперь имеет аргументы result и other_result, тогда мы можем написать<br>
    p.starmap(get_result, [(result_1, result_2), (other_result_1, other_result_2)])<br>
    Тогда при распаковке выполнятся два процесса, которые в качестве аргументов примут аргументы распакованных кортежей позиционно.
</p>
<h3>
    Value. Array
</h3>
<p>
    Еще два примитива для обработки одних данных из разны процессов - Value() и Array(). Особенность в том, что мы можем создать объекты этих
    примитивов, как-то повлиять на их значения из разных процессов и получить в итоге результат воздействия всех процессов.
</p>
<p>
    Array() представляет собой массив данных, Value() - одно единственное значение.
</p>
<p>
    Мы можем сразу в одной программе объявить эти примитивы и повзаимодействовать с ними
</p>
<p>
    Создадим объекты каждого примитива и функции внутри класса для их изменения. Первым делом посмотрим на объявление<br>
    array = multiprocessing.Array('i', rand_number), i - целочисленный тип данных.<br>
    value = multiprocessing.Value('d', rand_number), d - вещественный тип данных<br>
    Объявление типа данных, при создании этих объектов, обязательно.
</p>
<p>
    Для объекта Value() будем использовать статическую функцию для проверки на четность, процесс при этом используется отдельный.
</p>
<p>
    Для объекта Array() будет использовать несколько процессов и каждый из процессов будет добавлять в array свой id.
</p>
<p>
    Таким образом, создаются эти объекты внутри основного процесса, а используются внутри дочерних, при этом изменения над объектами внутри дочерних классах отражается в итоговом результате.
</p>
<h3>
    Manager
</h3>
<p>
    Следующий примитив - Manager(), идейно слегка напоминает идею Value() и Array().<br>
    Через объект Manager() мы также можем создавать хранилище для какого-нибудь типа данных и изменять его через другие процессы.
</p>
<p>
    Manager() поддерживает различные типы данных, на примере создается тип список, точно также можно создавать и прочие типы. Из стандартных поддерживается список и
    словарь, помимо стандартных поддерживаются примитивы multiprocessing, например Lock(), Queue(), Barrier() и прочие.
</p>
<p>
    Создавать из одно менеджера можно одновременно объявлять хранилища под разные данные, и задействовать их в том процессе, в котором это требуется.
</p>
<p>
    На примере объявляем хранилище типа список, изначально он пустой, но после того как мы в двух разных процессов положили в него значение список изменился и стал содержать в
    себе два этих значения. Соответственно, если запустить больше процессов, то количество значений в списке будет равно количеству запущенных процессов.
</p>
<p>
    Еще один тип менеджера - BaseManager(). Идея в том, что мы создаем сервер с зарегистрированными функциями и создаем клиентов, которые могу подключаться к этому серверу
    и использовать его функции, но при условии, что сервер в данный момент запущен и клиент знает данные для подключения.
</p>
<p>
    Создадим два файла.<br>
    В multiprocessing_class_basemanager.py регистрируем сервер.<br>
    В multiprocessing_class_basemanager_client.py подключаемся к севреру.
</p>
<p>
    Метод .register() принимает имя функции, которое мы придумываем сами, а в параметр callable передаем функцию ассоциированную с этим именем.<br>
    Регистрируется сервер с адресов, который состоит из ip и порта, и пароля в бинарном формате.<br>
    .get_server() - для получения зарегистрированного сервера<br>
    .serve_forever() - для запуска.
</p>
<p>
    При запуске multiprocessing_class_basemanager.py мы видим, что сервер запущен и программа не закрывается, пока мы ее не закроем вручную. Через .address я
    вывел значение адреса, просто, чтобы вывод не был пустым.
</p>
<p>
    Соединение открыто, можем писать клиентов и подключать к серверу.
</p>
<p>
    В multiprocessing_class_basemanager_client.py нам также необходимо объявить, что за метод мы будем использовать на сервере куда хотим подключиться, далее передаем адрес
    клиент, ip в данном случае обязателен и пароль.
</p>
<p>
    Если сервер запущен и пароль передан верно мы успешно к нему подключимся. Результат интересующей функции можно забрать в переменную и вывести на печать, разумеется на
    сервере должна быть зарегистрирована функция с таким именем.
</p>
<p>
    Теперь, пока сервер включен, мы можем каждый раз запускать файл multiprocessing_class_basemanager_client.py и получать случайное число от 1 до 100.
    Либо мы можем создать несколько клиентов и каждый сможет обращаться к серверу и получать случайное число.
</p>

<p>
    Обновлен материал "Python. Асинхронность. Многопоточность. Многопроцессорность.". Добавлена информация про:
    - aiohttp
    - некоторое api asincio
    - библиотеку threading
    - библиотеку multiprocessing
</p>